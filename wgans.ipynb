{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8266159,"sourceType":"datasetVersion","datasetId":4907114}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Importing the necessary libraries*","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# For loading and transforming data\nimport cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, precision_score\n\n# Additional utilities\nfrom torch.optim import Adam\nfrom torch.nn import Conv2d, ConvTranspose2d, LeakyReLU, BatchNorm2d\nfrom torchvision.utils import save_image","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:33:46.286846Z","iopub.execute_input":"2024-04-30T04:33:46.288514Z","iopub.status.idle":"2024-04-30T04:33:56.527506Z","shell.execute_reply.started":"2024-04-30T04:33:46.288449Z","shell.execute_reply":"2024-04-30T04:33:56.525762Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# *Creating the dataset for the training of GANS*","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Base directory where the original folders are located\nbase_dir = '/kaggle/input/dataset/Classifier Data'\n\n# New directory where the combined images will be located\nnew_base_dir = '/kaggle/working/images/'\n\n# Create new directories if they don't exist\nos.makedirs(os.path.join(new_base_dir, 'Diseased'), exist_ok=True)\nos.makedirs(os.path.join(new_base_dir, 'No_Disease'), exist_ok=True)\n\n# Categories and diseases\ncategories = ['train', 'val']\ndiseases = ['Disease_Present', 'No_Disease']\n\n# Copy the files\nfor cat in categories:\n    for disease in diseases:\n        # Directory where the current images are located\n        old_dir = os.path.join(base_dir, cat, disease)\n        \n        # Directory where the images are going to be moved to\n        new_dir_name = 'Diseased' if disease == 'Disease_Present' else 'No_Disease'\n        new_dir = os.path.join(new_base_dir, new_dir_name)\n\n        # Copy each file\n        for filename in os.listdir(old_dir):\n            old_file = os.path.join(old_dir, filename)\n            new_file = os.path.join(new_dir, filename)\n            \n            # Check if the file already exists, if so, skip or rename\n            if not os.path.exists(new_file):\n                shutil.copy(old_file, new_file)  # Copy the file\n            else:\n                # If a file with the same name exists, append an extra identifier before the extension\n                base, extension = os.path.splitext(new_file)\n                new_filename = base + '_duplicate' + extension\n                shutil.copy(old_file, new_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:33:56.530439Z","iopub.execute_input":"2024-04-30T04:33:56.531271Z","iopub.status.idle":"2024-04-30T04:36:18.470095Z","shell.execute_reply.started":"2024-04-30T04:33:56.531219Z","shell.execute_reply":"2024-04-30T04:36:18.466695Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Generator Network**","metadata":{}},{"cell_type":"code","source":"# Creating the generator architecture\nclass Generator(nn.Module):\n    def __init__(self, z_dim, img_shape, n_classes):\n        super(Generator, self).__init__()\n        self.img_shape = img_shape\n        self.label_embedding = nn.Embedding(n_classes, n_classes)\n\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(z_dim + n_classes, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, self.img_shape[0], 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        # Embed labels and concatenate with the noise vector\n        label_emb = self.label_embedding(labels)  # Transform labels into embeddings\n        label_emb = label_emb.unsqueeze(2).unsqueeze(3)  # Reshape to match the batch and noise dimensions\n        z = z.unsqueeze(2).unsqueeze(3)  # Reshape z to match the batch and label dimensions\n\n        # Concatenate noise vector z and label embeddings along the channel dimension\n        input_gen = torch.cat([z, label_emb], dim=1)\n\n        # Generate an image from the noise vector and labels\n        output = self.model(input_gen)\n        output = output.view(-1, *self.img_shape)  # Reshape to the output image size (C, H, W)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:36:18.476157Z","iopub.execute_input":"2024-04-30T04:36:18.476755Z","iopub.status.idle":"2024-04-30T04:36:18.498377Z","shell.execute_reply.started":"2024-04-30T04:36:18.476706Z","shell.execute_reply":"2024-04-30T04:36:18.496790Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Discrminator Network**","metadata":{}},{"cell_type":"code","source":"# Creating the discriminator architecture\nclass Discriminator(nn.Module):\n    def __init__(self, img_shape, n_classes):\n        super(Discriminator, self).__init__()\n        nc = img_shape[0]  # Number of channels in the images\n\n        self.label_embedding = nn.Embedding(n_classes, n_classes)\n        self.model = nn.Sequential(\n            nn.Conv2d(nc + n_classes, 64, 4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(1024, 2048, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(2048),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(2048, 1, 4, stride=1, padding=0, bias=False),\n            nn.Flatten()\n        )\n\n    def forward(self, img, labels):\n        label_embedding = self.label_embedding(labels)\n        label_embedding = label_embedding.view(-1, label_embedding.size(1), 1, 1)\n        label_embedding = label_embedding.repeat(1, 1, img.shape[2], img.shape[3])\n        img = torch.cat((img, label_embedding), 1)\n\n        return self.model(img)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:36:18.499763Z","iopub.execute_input":"2024-04-30T04:36:18.500199Z","iopub.status.idle":"2024-04-30T04:36:18.522711Z","shell.execute_reply.started":"2024-04-30T04:36:18.500166Z","shell.execute_reply":"2024-04-30T04:36:18.520925Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Setting of hyperparameters and initialising the networks**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Hyperparameters\nz_dim = 100\nimg_size = 256\nimg_channels = 3  # RGB images\nn_classes = 2  # Diseased or not\nlr_d = 2e-6  # Lower learning rate for better stability\nlr_g = 2e-4\nbatch_size = 32\nepochs = 100\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Initialize generator and discriminator\nimg_shape = (img_channels, img_size, img_size)\ngenerator = Generator(z_dim=z_dim, img_shape=img_shape, n_classes=n_classes).to(device)\ndiscriminator = Discriminator(img_shape=img_shape, n_classes=n_classes).to(device)\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.0, 0.9))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.0, 0.9))\n\nadversarial_loss = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:36:27.031994Z","iopub.execute_input":"2024-04-30T04:36:27.033628Z","iopub.status.idle":"2024-04-30T04:36:27.799187Z","shell.execute_reply.started":"2024-04-30T04:36:27.033568Z","shell.execute_reply":"2024-04-30T04:36:27.797410Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Image Transformations**","metadata":{}},{"cell_type":"code","source":"# Image transformations\nfrom torchvision import datasets, transforms\ntransform = transforms.Compose([\n    transforms.Resize(img_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Data loaders for your dataset\ndataloader = DataLoader(\n    datasets.ImageFolder('/kaggle/working/images/', transform=transform),\n    batch_size=batch_size,\n    shuffle=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:36:30.637325Z","iopub.execute_input":"2024-04-30T04:36:30.637823Z","iopub.status.idle":"2024-04-30T04:36:30.700747Z","shell.execute_reply.started":"2024-04-30T04:36:30.637788Z","shell.execute_reply":"2024-04-30T04:36:30.699023Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Training Loop**","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom PIL import Image\nimport os\n\n# Function to display and save generated images\ndef show_and_save_generated(imgs, labels, epoch, generated_images_dir, num_images=10):\n    imgs = (imgs + 1) / 2  # Rescale images from [-1,1] to [0,1]\n    grid = make_grid(imgs[:num_images], nrow=5).detach().cpu().numpy()\n    grid = np.transpose(grid, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n\n    # Display the grid of images\n    plt.figure(figsize=(10, 5))\n    plt.imshow(grid)\n    plt.axis('off')\n    plt.show()\n\n    # Save individual images\n    for i, img in enumerate(imgs):\n        class_label = 'No_Disease' if labels[i].item() == 0 else 'Diseased'\n        class_dir = os.path.join(generated_images_dir, class_label)\n        os.makedirs(class_dir, exist_ok=True)  # Create the class directory if it doesn't exist\n        image_path = os.path.join(class_dir, f'epoch_{epoch}_image_{i}.png')\n        save_image(img, image_path)\n\ndef compute_gradient_penalty(D, real_samples, fake_samples, labels):\n    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n    # Random weight term for interpolation between real and fake samples\n    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n    alpha = alpha.expand_as(real_samples)\n    # Get random interpolation between real and fake samples\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates, labels)\n    \n    # Create a tensor for 'grad_outputs' filled with ones, which is required for the gradient computation\n    grad_outputs = torch.ones_like(d_interpolates, device=real_samples.device)\n    \n    # Get gradient w.r.t. interpolates\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=grad_outputs,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n\n\n# Define a directory to save model checkpoints\ncheckpoint_dir = '/kaggle/working/checkpoints_wgans/'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Define a directory to save generated images\ngenerated_images_dir = '/kaggle/working/generated_images_wgans/'\nos.makedirs(generated_images_dir, exist_ok=True)\n\n\n# Define the number of critic iterations per generator iteration\ncritic_iterations = 3\nlambda_gp = 8  # Gradient penalty lambda hyperparameter\n\n# Define a dictionary to store epoch losses\nepoch_losses = {}\n\n# Training loop\nfor epoch in range(epochs):\n    epoch_g_loss = 0.0  # Initialize epoch generator loss\n    epoch_d_loss = 0.0\n    \n    for i, (imgs, labels) in enumerate(dataloader):\n        \n        real_imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n\n        # Sample noise as generator input\n        z = torch.randn(imgs.size(0), z_dim).to(device)\n        gen_labels = torch.randint(0, n_classes, (imgs.size(0),)).to(device)\n\n        # Generate a batch of images\n        fake_imgs = generator(z, gen_labels)\n\n        # Real images\n        real_validity = discriminator(real_imgs, labels)\n        # Fake images\n        fake_validity = discriminator(fake_imgs.detach(), gen_labels)\n        # Gradient penalty\n        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data, labels)\n\n        # Discriminator loss\n        d_loss = fake_validity.mean() - real_validity.mean() + lambda_gp * gradient_penalty\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        optimizer_G.zero_grad()\n        if i % critic_iterations == 0:\n            # Train the generator every critic_iterations steps\n            # -----------------\n            #  Train Generator\n            # -----------------\n            # Generate a batch of images\n            gen_imgs = generator(z, gen_labels)\n            # Loss measures generator's ability to fool the discriminator\n            g_loss = -discriminator(gen_imgs, gen_labels).mean()\n\n            g_loss.backward()\n            optimizer_G.step()\n        \n    # Calculate average losses for this epoch\n        avg_g_loss = epoch_g_loss / len(dataloader)\n        avg_d_loss = epoch_d_loss / len(dataloader)\n        \n        # Store the epoch number and losses in the dictionary\n        epoch_losses[epoch] = {'generator_loss': avg_g_loss, 'discriminator_loss': avg_d_loss}\n\n        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n    # Check pointing for every epoch\n    if epoch % 10 == 0:\n        torch.save(generator.state_dict(), os.path.join(checkpoint_dir, f'generator_epoch_{epoch}.pth'))\n        torch.save(discriminator.state_dict(), os.path.join(checkpoint_dir, f'discriminator_epoch_{epoch}.pth'))\n        # Generate and save example images\n        with torch.no_grad():\n            z_example = torch.randn(10, z_dim).to(device)  # Generate 10 random noise vectors\n            gen_labels_example = torch.randint(0, n_classes, (10,)).to(device)  # Generate random labels\n            gen_imgs_example = generator(z_example, gen_labels_example)\n            show_and_save_generated(gen_imgs_example, gen_labels_example, epoch, generated_images_dir, num_images=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(valid.shape)\nprint(validity.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:37:47.881343Z","iopub.execute_input":"2024-04-30T04:37:47.881946Z","iopub.status.idle":"2024-04-30T04:37:47.891502Z","shell.execute_reply.started":"2024-04-30T04:37:47.881890Z","shell.execute_reply":"2024-04-30T04:37:47.889522Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([32, 1])\ntorch.Size([32, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n# Convert epoch_losses dictionary to lists\nepochs_list = list(epoch_losses.keys())\ngenerator_losses_list = [entry['generator_loss'] for entry in epoch_losses.values()]\ndiscriminator_losses_list = [entry['discriminator_loss'] for entry in epoch_losses.values()]\n\n# Create Plotly figure\nfig = go.Figure()\n\n# Add generator loss trace\nfig.add_trace(go.Scatter(x=epochs_list, y=generator_losses_list, mode='lines', name='Generator Loss'))\n\n# Add discriminator loss trace\nfig.add_trace(go.Scatter(x=epochs_list, y=discriminator_losses_list, mode='lines', name='Discriminator Loss'))\n\n# Update layout\nfig.update_layout(\n    title='Generator and Discriminator Losses Over Epochs',\n    xaxis_title='Epoch',\n    yaxis_title='Loss',\n    legend=dict(x=0, y=1),\n)\n\n# Show plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T04:43:22.910825Z","iopub.status.idle":"2024-04-30T04:43:22.911357Z","shell.execute_reply.started":"2024-04-30T04:43:22.911130Z","shell.execute_reply":"2024-04-30T04:43:22.911151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}