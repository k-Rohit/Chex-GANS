{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7972368,"sourceType":"datasetVersion","datasetId":4691223}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Core libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# For loading and transforming data\nimport cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, precision_score\n\n# Additional utilities\nfrom torch.optim import Adam\nfrom torch.nn import Conv2d, ConvTranspose2d, LeakyReLU, BatchNorm2d\nfrom torchvision.utils import save_image","metadata":{"execution":{"iopub.status.busy":"2024-05-01T04:38:58.902194Z","iopub.execute_input":"2024-05-01T04:38:58.902603Z","iopub.status.idle":"2024-05-01T04:39:07.904771Z","shell.execute_reply.started":"2024-05-01T04:38:58.902573Z","shell.execute_reply":"2024-05-01T04:39:07.903205Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Base directory where the original folders are located\nbase_dir = '/kaggle/input/cv-project-detector/Classifier Data'\n\n# New directory where the combined images will be located\nnew_base_dir = '/kaggle/working/images/'\n\n# Create new directories if they don't exist\nos.makedirs(os.path.join(new_base_dir, 'Diseased'), exist_ok=True)\nos.makedirs(os.path.join(new_base_dir, 'No_Disease'), exist_ok=True)\n\n# Categories and diseases\ncategories = ['train', 'val']\ndiseases = ['Disease_Present', 'No_Disease']\n\n# Copy the files\nfor cat in categories:\n    for disease in diseases:\n        # Directory where the current images are located\n        old_dir = os.path.join(base_dir, cat, disease)\n        \n        # Directory where the images are going to be moved to\n        new_dir_name = 'Diseased' if disease == 'Disease_Present' else 'No_Disease'\n        new_dir = os.path.join(new_base_dir, new_dir_name)\n\n        # Copy each file\n        for filename in os.listdir(old_dir):\n            old_file = os.path.join(old_dir, filename)\n            new_file = os.path.join(new_dir, filename)\n            \n            # Check if the file already exists, if so, skip or rename\n            if not os.path.exists(new_file):\n                shutil.copy(old_file, new_file)  # Copy the file\n            else:\n                # If a file with the same name exists, append an extra identifier before the extension\n                base, extension = os.path.splitext(new_file)\n                new_filename = base + '_duplicate' + extension\n                shutil.copy(old_file, new_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T04:39:07.907059Z","iopub.execute_input":"2024-05-01T04:39:07.907993Z","iopub.status.idle":"2024-05-01T04:41:33.893844Z","shell.execute_reply.started":"2024-05-01T04:39:07.907933Z","shell.execute_reply":"2024-05-01T04:41:33.892420Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### CGans - 256 by 256","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_shape, n_classes):\n        super(Discriminator, self).__init__()\n        nc = img_shape[0]  # Number of channels in the images\n\n        self.label_embedding = nn.Embedding(n_classes, n_classes)\n        self.model = nn.Sequential(\n            # input is (nc) x 256 x 256\n            nn.Conv2d(nc + n_classes, 64, 4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size. (64) x 128 x 128\n            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size. (128) x 64 x 64\n            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size. (256) x 32 x 32\n            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size. (512) x 16 x 16\n            nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size. (1024) x 8 x 8\n            nn.Conv2d(1024, 2048, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(2048),\n            nn.LeakyReLU(0.2, inplace=True),\n            # State size. (2048) x 4 x 4\n            nn.Conv2d(2048, 1, 4, stride=1, padding=0, bias=False),\n            nn.Flatten(),\n            nn.Sigmoid()\n            # Output size. 1\n        )\n\n    def forward(self, img, labels):\n        # Concatenate label embedding and image to produce input\n        label_embedding = self.label_embedding(labels)\n        label_embedding = label_embedding.view(label_embedding.size(0), label_embedding.size(1), 1, 1)\n        label_embedding = label_embedding.repeat(1, 1, img.shape[2], img.shape[3])\n        img = torch.cat((img, label_embedding), 1)\n\n        return self.model(img)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T05:17:04.461752Z","iopub.execute_input":"2024-04-30T05:17:04.462027Z","iopub.status.idle":"2024-04-30T05:17:04.478458Z","shell.execute_reply.started":"2024-04-30T05:17:04.462003Z","shell.execute_reply":"2024-04-30T05:17:04.477739Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nimport torch.optim as optim\nz_dim = 100\nimg_size = 256\nimg_channels = 3  # or 3 for RGB images\nn_classes = 2  # Diseased or not\nlr = 3e-4\nbatch_size = 64\nepochs = 100\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\n# Initialize generator and discriminator\nimg_shape = (img_channels, img_size, img_size)\ngenerator = Generator(z_dim=z_dim, img_shape=img_shape, n_classes=n_classes).to(device)\ngenerator.label_embedding = generator.label_embedding.to(device)\ndiscriminator = Discriminator(img_shape=img_shape, n_classes=n_classes).to(device)\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T05:17:05.241227Z","iopub.execute_input":"2024-04-30T05:17:05.241637Z","iopub.status.idle":"2024-04-30T05:17:05.760338Z","shell.execute_reply.started":"2024-04-30T05:17:05.241602Z","shell.execute_reply":"2024-04-30T05:17:05.759400Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Image transformations\nfrom torchvision import datasets, transforms\ntransform = transforms.Compose([\n    transforms.Resize(img_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Data loaders for your dataset\ndataloader = DataLoader(\n    datasets.ImageFolder('/kaggle/working/images/', transform=transform),\n    batch_size=batch_size,\n    shuffle=True,\n)\n\n# Loss function\nadversarial_loss = torch.nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T05:17:05.761455Z","iopub.execute_input":"2024-04-30T05:17:05.761804Z","iopub.status.idle":"2024-04-30T05:17:05.815669Z","shell.execute_reply.started":"2024-04-30T05:17:05.761772Z","shell.execute_reply":"2024-04-30T05:17:05.814753Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Accessing class names\nclass_names = dataloader.dataset.classes\nprint(\"Class Names:\", class_names)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T05:17:05.816676Z","iopub.execute_input":"2024-04-30T05:17:05.816940Z","iopub.status.idle":"2024-04-30T05:17:06.277333Z","shell.execute_reply.started":"2024-04-30T05:17:05.816910Z","shell.execute_reply":"2024-04-30T05:17:06.276306Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Class Names: ['Diseased', 'No_Disease']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom PIL import Image\nimport os\n\n# Function to display and save generated images\ndef show_and_save_generated(imgs, labels, epoch, generated_images_dir, num_images=10):\n    imgs = (imgs + 1) / 2  # Rescale images from [-1,1] to [0,1]\n    grid = make_grid(imgs[:num_images], nrow=5).detach().cpu().numpy()\n    grid = np.transpose(grid, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n\n    # Display the grid of images\n    plt.figure(figsize=(10, 5))\n    plt.imshow(grid)\n    plt.axis('off')\n    plt.show()\n\n    # Save individual images\n    for i, img in enumerate(imgs):\n        class_label = 'Diseased' if labels[i].item() == 0 else 'No_Disease'\n        class_dir = os.path.join(generated_images_dir, class_label)\n        os.makedirs(class_dir, exist_ok=True)  # Create the class directory if it doesn't exist\n        image_path = os.path.join(class_dir, f'epoch_{epoch}_image_{i}.png')\n        save_image(img, image_path)\n\n# Define a directory to save model checkpoints\ncheckpoint_dir = '/kaggle/working/checkpoints_cgans/'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Define a directory to save generated images\ngenerated_images_dir = '/kaggle/working/generated_images_cgans/'\nos.makedirs(generated_images_dir, exist_ok=True)\n\n\n\nepoch_losses = {}\n# Training Loop\nfor epoch in range(epochs):\n    \n    epoch_g_loss = 0.0  # Initialize epoch generator loss\n    epoch_d_loss = 0.0  # Initialize epoch discriminator loss\n    for i, (imgs, labels) in enumerate(dataloader):\n        \n        valid = torch.ones(imgs.size(0), 1, requires_grad=False).to(device)\n        fake = torch.zeros(imgs.size(0), 1, requires_grad=False).to(device)\n        \n        # Configure input\n        real_imgs = imgs.to(device)\n        labels = labels.to(device)\n        \n        # -----------------\n        #  Train Generator\n        # -----------------\n        optimizer_G.zero_grad()\n\n        # Sample noise and labels as generator input\n        z = torch.randn(imgs.size(0), z_dim).to(device)\n        gen_labels = torch.randint(0, n_classes, (imgs.size(0),)).to(device)\n\n        # Generate a batch of images\n        gen_imgs = generator(z, gen_labels)\n\n        # Loss measures generator's ability to fool the discriminator\n        validity = discriminator(gen_imgs, gen_labels)\n        g_loss = adversarial_loss(validity, valid)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n        optimizer_D.zero_grad()\n\n        # Loss for real images\n        validity_real = discriminator(real_imgs, labels)\n        d_real_loss = adversarial_loss(validity_real, valid)\n\n        # Loss for fake images\n        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n        d_fake_loss = adversarial_loss(validity_fake, fake)\n\n        # Total discriminator loss\n        d_loss = (d_real_loss + d_fake_loss) / 2\n\n        d_loss.backward()\n        optimizer_D.step()\n        \n        epoch_g_loss += g_loss.item()\n        epoch_d_loss += d_loss.item()\n    \n    # Calculate average losses for this epoch\n    avg_g_loss = epoch_g_loss / len(dataloader)\n    avg_d_loss = epoch_d_loss / len(dataloader)\n    \n    epoch_losses[epoch] = {'generator_loss': avg_g_loss, 'discriminator_loss': avg_d_loss}\n        \n    print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n\n    # Save model checkpoints every 10 epochs and display grid of images\n    if epoch % 10 == 0:\n        torch.save(generator.state_dict(), os.path.join(checkpoint_dir, f'generator_epoch_{epoch}.pth'))\n        torch.save(discriminator.state_dict(), os.path.join(checkpoint_dir, f'discriminator_epoch_{epoch}.pth'))\n\n        # Generate and save example images\n        with torch.no_grad():\n            z_example = torch.randn(10, z_dim).to(device)  # Generate 10 random noise vectors\n            gen_labels_example = torch.randint(0, n_classes, (10,)).to(device)  # Generate random labels\n            gen_imgs_example = generator(z_example, gen_labels_example)\n            show_and_save_generated(gen_imgs_example, gen_labels_example, epoch, generated_images_dir, num_images=10)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n# Extract generator and discriminator losses\ngenerator_losses = [epoch_losses[epoch]['generator_loss'] for epoch in range(epochs)]\ndiscriminator_losses = [epoch_losses[epoch]['discriminator_loss'] for epoch in range(epochs)]\n\n# Create Plotly figure\nfig = go.Figure()\n\n# Add generator loss trace\nfig.add_trace(go.Scatter(x=list(range(epochs)), y=generator_losses, mode='lines', name='Generator Loss'))\n\n# Add discriminator loss trace\nfig.add_trace(go.Scatter(x=list(range(epochs)), y=discriminator_losses, mode='lines', name='Discriminator Loss'))\n\n# Update layout\nfig.update_layout(title='Generator and Discriminator Losses vs Epoch',\n                  xaxis_title='Epoch',\n                  yaxis_title='Loss')\n\n# Show plot\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-30T08:44:07.144783Z","iopub.execute_input":"2024-04-30T08:44:07.145138Z","iopub.status.idle":"2024-04-30T08:44:07.540307Z","shell.execute_reply.started":"2024-04-30T08:44:07.145101Z","shell.execute_reply":"2024-04-30T08:44:07.539441Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"5818aea2-f905-4003-822f-5f310a8f4035\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5818aea2-f905-4003-822f-5f310a8f4035\")) {                    Plotly.newPlot(                        \"5818aea2-f905-4003-822f-5f310a8f4035\",                        [{\"mode\":\"lines\",\"name\":\"Generator Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[4.545907090647497,1.4988916964688594,1.2858803013636582,1.4416702410233193,1.4498071660814078,1.4828507838149865,1.438472672433093,1.6718220235644907,2.067966779693961,2.1569557647774187,2.2223892937535825,1.8173693153953208,2.345991987357105,2.275932516200819,2.286935867228802,2.7915330460106116,3.446605888924197,4.032177705263746,3.3437486527406652,3.8277453613885934,4.107303723045018,4.421894880983492,3.9673949534113966,4.434214815650299,3.921472563273341,4.517016736918763,3.966188544250579,3.804819439848264,4.241128142113271,4.602576420358989,5.241703242063522,4.759430523205927,5.04956016484378,4.7317395083930185,5.1167822411094885,4.952644697134046,4.972686581934492,5.586901094386543,5.123573780491732,5.334124815636787,5.213648797809213,6.306772144808286,5.491295746271161,5.692678095637888,6.5041425133096995,6.206629215807155,6.747939773227857,6.108173675943112,5.6439148688661875,6.254308270803397],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Discriminator Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"y\":[0.6660355575731955,0.7467780681192011,0.7052867028160371,0.6427787332863047,0.6732227928612543,0.6326014963181122,0.6192957188772119,0.5917884719328604,0.6129092181074447,0.5302979613019936,0.5398136310927246,0.5900071912269661,0.48965500292462716,0.5481022451666818,0.47754628424519213,0.45019089456215716,0.31553228605754563,0.27125968339353584,0.3677268194682572,0.31086026738141326,0.30554762424266746,0.2775684915958782,0.3183296071095527,0.22136176690253415,0.3100887492393562,0.23634791519810056,0.2642310610634909,0.20568056989704137,0.2326153419925359,0.24211789513735668,0.2327023123498952,0.22088909125211986,0.16525120017510178,0.18084854025231756,0.18766612496119048,0.16824946127008591,0.15097847644853796,0.1608840632033062,0.21184793691466347,0.11974350617527017,0.14611590346253978,0.1520377770876107,0.11685329709586728,0.12008469293325924,0.11863759094874536,0.12187047107031306,0.11825265451920712,0.13810534256723864,0.0929851621200425,0.1607668647486497],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Generator and Discriminator Losses vs Epoch\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('5818aea2-f905-4003-822f-5f310a8f4035');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]}]}