{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7972368,"sourceType":"datasetVersion","datasetId":4691223}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Core libraries\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n# For loading and transforming data\nimport cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, precision_score\n\n# Additional utilities\nfrom torch.optim import Adam\nfrom torch.nn import Conv2d, ConvTranspose2d, LeakyReLU, BatchNorm2d\nfrom torchvision.utils import save_image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T05:00:11.683239Z","iopub.execute_input":"2024-05-01T05:00:11.684086Z","iopub.status.idle":"2024-05-01T05:00:19.115314Z","shell.execute_reply.started":"2024-05-01T05:00:11.684052Z","shell.execute_reply":"2024-05-01T05:00:19.114302Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\n# Base directory where the original folders are located\nbase_dir = '/kaggle/input/cv-project-detector/Classifier Data'\n\n# New directory where the combined images will be located\nnew_base_dir = '/kaggle/working/images/'\n\n# Create new directories if they don't exist\nos.makedirs(os.path.join(new_base_dir, 'Diseased'), exist_ok=True)\nos.makedirs(os.path.join(new_base_dir, 'No_Disease'), exist_ok=True)\n\n# Categories and diseases\ncategories = ['train', 'val']\ndiseases = ['Disease_Present', 'No_Disease']\n\n# Copy the files\nfor cat in categories:\n    for disease in diseases:\n        # Directory where the current images are located\n        old_dir = os.path.join(base_dir, cat, disease)\n        \n        # Directory where the images are going to be moved to\n        new_dir_name = 'Diseased' if disease == 'Disease_Present' else 'No_Disease'\n        new_dir = os.path.join(new_base_dir, new_dir_name)\n\n        # Copy each file\n        for filename in os.listdir(old_dir):\n            old_file = os.path.join(old_dir, filename)\n            new_file = os.path.join(new_dir, filename)\n            \n            # Check if the file already exists, if so, skip or rename\n            if not os.path.exists(new_file):\n                shutil.copy(old_file, new_file)  # Copy the file\n            else:\n                # If a file with the same name exists, append an extra identifier before the extension\n                base, extension = os.path.splitext(new_file)\n                new_filename = base + '_duplicate' + extension\n                shutil.copy(old_file, new_filename)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:00:19.117251Z","iopub.execute_input":"2024-05-01T05:00:19.117749Z","iopub.status.idle":"2024-05-01T05:08:19.853547Z","shell.execute_reply.started":"2024-05-01T05:00:19.117715Z","shell.execute_reply":"2024-05-01T05:08:19.852646Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim, img_shape, n_classes):\n        super(Generator, self).__init__()\n        self.img_shape = img_shape\n        self.label_embedding = nn.Embedding(n_classes, n_classes)\n\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(z_dim + n_classes, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(32, 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, self.img_shape[0], 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, z, labels):\n        # Embed labels and concatenate with the noise vector\n        label_emb = self.label_embedding(labels)  # Transform labels into embeddings\n        label_emb = label_emb.unsqueeze(2).unsqueeze(3)  # Reshape to match the batch and noise dimensions\n        z = z.unsqueeze(2).unsqueeze(3)  # Reshape z to match the batch and label dimensions\n\n        # Concatenate noise vector z and label embeddings along the channel dimension\n        input_gen = torch.cat([z, label_emb], dim=1)\n\n        # Generate an image from the noise vector and labels\n        output = self.model(input_gen)\n        output = output.view(-1, *self.img_shape)  # Reshape to the output image size (C, H, W)\n\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:08:19.854875Z","iopub.execute_input":"2024-05-01T05:08:19.855249Z","iopub.status.idle":"2024-05-01T05:08:19.867490Z","shell.execute_reply.started":"2024-05-01T05:08:19.855219Z","shell.execute_reply":"2024-05-01T05:08:19.866623Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_shape, n_classes):\n        super(Discriminator, self).__init__()\n        nc = img_shape[0]  # Number of channels in the images\n\n        self.label_embedding = nn.Embedding(n_classes, n_classes)\n        self.model = nn.Sequential(\n            nn.Conv2d(nc + n_classes, 64, 4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(1024, 2048, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(2048),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(2048, 1, 4, stride=1, padding=0, bias=False),\n            nn.Flatten()\n        )\n\n    def forward(self, img, labels):\n        label_embedding = self.label_embedding(labels)\n        label_embedding = label_embedding.view(-1, label_embedding.size(1), 1, 1)\n        label_embedding = label_embedding.repeat(1, 1, img.shape[2], img.shape[3])\n        img = torch.cat((img, label_embedding), 1)\n\n        return self.model(img)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:08:19.869596Z","iopub.execute_input":"2024-05-01T05:08:19.869977Z","iopub.status.idle":"2024-05-01T05:08:20.302504Z","shell.execute_reply.started":"2024-05-01T05:08:19.869946Z","shell.execute_reply":"2024-05-01T05:08:20.301639Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Hyperparameters\nz_dim = 100\nimg_size = 256\nimg_channels = 3  # RGB images\nn_classes = 2  # Diseased or not\nlr_d = 2e-6  # Lower learning rate for better stability\nlr_g = 2e-4\nbatch_size = 32\nepochs = 100\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Initialize generator and discriminator\nimg_shape = (img_channels, img_size, img_size)\ngenerator = Generator(z_dim=z_dim, img_shape=img_shape, n_classes=n_classes).to(device)\ndiscriminator = Discriminator(img_shape=img_shape, n_classes=n_classes).to(device)\n\n# Optimizers\noptimizer_G = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.0, 0.9))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.0, 0.9))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:24:12.547205Z","iopub.execute_input":"2024-05-01T05:24:12.547549Z","iopub.status.idle":"2024-05-01T05:24:13.060650Z","shell.execute_reply.started":"2024-05-01T05:24:12.547522Z","shell.execute_reply":"2024-05-01T05:24:13.059675Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Image transformations\nfrom torchvision import datasets, transforms\ntransform = transforms.Compose([\n    transforms.Resize(img_size),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5])\n])\n\n# Data loaders for your dataset\ndataloader = DataLoader(\n    datasets.ImageFolder('/kaggle/working/images/', transform=transform),\n    batch_size=batch_size,\n    shuffle=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T05:24:13.889084Z","iopub.execute_input":"2024-05-01T05:24:13.889780Z","iopub.status.idle":"2024-05-01T05:24:13.938087Z","shell.execute_reply.started":"2024-05-01T05:24:13.889728Z","shell.execute_reply":"2024-05-01T05:24:13.937326Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nfrom torchvision.utils import make_grid\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\n# Assume the setup for generator, discriminator, dataloader, and optimizer_D, optimizer_G are defined\n# as well as variables like device, z_dim, n_classes, etc.\n\n# Define a directory to save model checkpoints and generated images\ncheckpoint_dir = '/kaggle/working/checkpoints_wgans/'\nos.makedirs(checkpoint_dir, exist_ok=True)\ngenerated_images_dir = '/kaggle/working/generated_images_wgans/'\nos.makedirs(generated_images_dir, exist_ok=True)\n\ndef show_and_save_generated(imgs, labels, epoch, generated_images_dir, num_images=10):\n    imgs = (imgs + 1) / 2  # Rescale images from [-1,1] to [0,1]\n    grid = make_grid(imgs[:num_images], nrow=5).detach().cpu().numpy()\n    grid = np.transpose(grid, (1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(grid)\n    plt.axis('off')\n    plt.show()\n\n    for i, img in enumerate(imgs.cpu()):  # Move tensors to CPU before converting\n        img = Image.fromarray((img.numpy() * 255).astype('uint8'), mode='RGB')\n        class_label = 'Disease' if labels[i].item() == 0 else 'No_Diseased'\n        class_dir = os.path.join(generated_images_dir, class_label)\n        os.makedirs(class_dir, exist_ok=True)\n        image_path = os.path.join(class_dir, f'epoch_{epoch}_image_{i}.png')\n        img.save(image_path)\n\n\ndef compute_gradient_penalty(D, real_samples, fake_samples, labels):\n    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n    alpha = alpha.expand_as(real_samples)\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates, labels)\n    grad_outputs = torch.ones_like(d_interpolates, device=real_samples.device)\n    gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=grad_outputs, create_graph=True, retain_graph=True, only_inputs=True)[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n\nepoch_losses = {'generator': [], 'discriminator': []}\ncritic_iterations = 3\nlambda_gp = 8\n\nfor epoch in range(epochs):\n    epoch_g_loss = 0.0\n    epoch_d_loss = 0.0\n    for i, (imgs, labels) in enumerate(dataloader):\n        real_imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        # Training Discriminator\n        optimizer_D.zero_grad()\n        z = torch.randn(imgs.size(0), z_dim).to(device)\n        gen_labels = torch.randint(0, n_classes, (imgs.size(0),)).to(device)\n        fake_imgs = generator(z, gen_labels)\n        real_validity = discriminator(real_imgs, labels)\n        fake_validity = discriminator(fake_imgs.detach(), gen_labels)\n        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs, labels)\n        d_loss = fake_validity.mean() - real_validity.mean() + lambda_gp * gradient_penalty\n        d_loss.backward()\n        optimizer_D.step()\n        epoch_d_loss += d_loss.item()\n\n        # Training Generator\n        if i % critic_iterations == 0:\n            optimizer_G.zero_grad()\n            gen_imgs = generator(z, gen_labels)\n            g_loss = -discriminator(gen_imgs, gen_labels).mean()\n            g_loss.backward()\n            optimizer_G.step()\n            epoch_g_loss += g_loss.item()\n\n    # Average losses for the epoch\n    epoch_losses['generator'].append(epoch_g_loss / (len(dataloader) // critic_iterations + 1))\n    epoch_losses['discriminator'].append(epoch_d_loss / len(dataloader))\n\n    # Output training stats\n    print(f\"[Epoch {epoch}/{epochs}] [D loss: {epoch_d_loss / len(dataloader)}] [G loss: {epoch_g_loss / (len(dataloader) // critic_iterations + 1)}]\")\n\n    if epoch % 10 == 0:\n        torch.save(generator.state_dict(), os.path.join(checkpoint_dir, f'generator_epoch_{epoch}.pth'))\n        torch.save(discriminator.state_dict(), os.path.join(checkpoint_dir, f'discriminator_epoch_{epoch}.pth'))\n        with torch.no_grad():\n            z_example = torch.randn(10, z_dim).to(device)\n            gen_labels_example = torch.randint(0, n_classes, (10,)).to(device)\n            gen_imgs_example = generator(z_example, gen_labels_example)\n            show_and_save_generated(gen_imgs_example, gen_labels_example, epoch, generated_images_dir, num_images=10)\n\n# Plotly for loss visualization\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(epochs)), y=epoch_losses['generator'], mode='lines+markers', name='Generator Loss'))\nfig.add_trace(go.Scatter(x=list(range(epochs)), y=epoch_losses['discriminator'], mode='lines+markers', name='Discriminator Loss'))\nfig.update_layout(title='Training Losses', xaxis_title='Epoch', yaxis_title='Loss', legend_title='Component')\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
